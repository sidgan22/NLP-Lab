{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "![Question 1](q1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sidga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A chatbot is a computer program that is designed to simulate human conversation . Users communicate with these tools using a chat interface or via voice , just like they would converse with another person . MedWhat-One of my favorite pastimes is radically misdiagnosis myself with life-threatening illnesses on medical webster ( often in the wee hours of the night when I can a t sleep ) . If you a re the kind of person who has weird bookmaker for similar reasons , it might be worth checking out MedWhat.This chariot aims to make medical diagnoses faster , easier , and more transparent for both patients and physicians a think of it like an intelligent version of weird that you can talk to . medway is powered by a sophisticated machine learning system that offers increasingly accurate responses to user questions based on behaviors that it a learns a by interacting with human beings . In many ways , medway is much closer to a virtual assistant ( like goole Now ) rather than a conversational agent . It also represents an exciting field of chariot development that pairs intelligent NLP systems with machine learning technology to offer users an accurate and responsive experience .\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f=open('output.txt','r',encoding=\"utf8\")\n",
    "raw=f.read()\n",
    "raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'chatbot', 'is', 'a', 'computer']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = nltk.word_tokenize(raw)\n",
    "words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 10),\n",
       " ('.', 7),\n",
       " ('to', 6),\n",
       " ('of', 6),\n",
       " ('is', 5),\n",
       " ('that', 5),\n",
       " ('with', 5),\n",
       " (',', 5),\n",
       " ('like', 3),\n",
       " ('the', 3)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Question 2 & 3: 10 most frequently used tokens and their term frequencies\n",
    "freq_dist = FreqDist(words)\n",
    "freq_dist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "respons\n",
      "develop\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "lanc = LancasterStemmer()\n",
    "print(porter.stem('responses'))\n",
    "print(porter.stem('development'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  tools || Lemmatized:  tool\n",
      "Original:  pastimes || Lemmatized:  pastime\n",
      "Original:  illnesses || Lemmatized:  illness\n",
      "Original:  hours || Lemmatized:  hour\n",
      "Original:  has || Lemmatized:  ha\n",
      "Original:  reasons || Lemmatized:  reason\n",
      "Original:  aims || Lemmatized:  aim\n",
      "Original:  diagnoses || Lemmatized:  diagnosis\n",
      "Original:  patients || Lemmatized:  patient\n",
      "Original:  physicians || Lemmatized:  physician\n",
      "Original:  offers || Lemmatized:  offer\n",
      "Original:  responses || Lemmatized:  response\n",
      "Original:  questions || Lemmatized:  question\n",
      "Original:  behaviors || Lemmatized:  behavior\n",
      "Original:  beings || Lemmatized:  being\n",
      "Original:  ways || Lemmatized:  way\n",
      "Original:  pairs || Lemmatized:  pair\n",
      "Original:  systems || Lemmatized:  system\n",
      "Original:  users || Lemmatized:  user\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "wnLem = WordNetLemmatizer()\n",
    "for word in nltk.word_tokenize(raw):\n",
    "    if(word != wnLem.lemmatize(word)):\n",
    "        print(\"Original: \",word,end=\"\")\n",
    "        print(\" || Lemmatized: \",wnLem.lemmatize(word))\n",
    "#     print(\"Original: \",word,\"\\n Lemmatized: \",wnLem.lemmatize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INLTK - APIs\n",
    "\n",
    "\n",
    "## Get Embedding Vectors\n",
    "    - This returns an array of “Embedding vectors”, containing 400 Dimensional representation for every token in the text.\n",
    "    \n",
    "## Remove Foreign Languages\n",
    "    - Every word other than that of host language will become <unk> and ▁ signifies space character\n",
    "    \n",
    "## Get Sentence Encoding\n",
    "\n",
    "    - Can be used to get_sentence_encoding returns 400 dimensional encoding of the sentence from ULMFiT LM Encoder of <code-of-language> trained.\n",
    "  \n",
    "## Get Sentence Similarity\n",
    "    \n",
    "    - Can be used to find the similarity between two sentences.\n",
    "    - The similarity of encodings is calculated by using cmp function whose default is cosine similarity\n",
    "\n",
    "## Identify Languages\n",
    "\n",
    "    - It can be used to identify the language of a given text.\n",
    "    - reset_language_identifying_models() # only if you've updated iNLTK version identify_language(text)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
